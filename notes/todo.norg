@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2023-12-06T14:23:14+0100
@end

* TODO
  - Verify enhancements:
  -- ( ) ReLU entropy
  -- ( ) Action-specific weighted entropy
  -- ( ) Critic network
  -- ( ) Multiple batches per epoch
  -- ( ) PPO loss (if multiple batches)
  -- ( ) Splitted actor/critic models
  -- ( ) Mixed reward sources
  -- ( ) Use the n_steps input
  - ( ) Never-ending episodes with appropriate rewards.
  - ( ) Delta-max reward.
  - ( ) Composite rewards.
  - ( ) Critic network.
  -- ( ) With help for reward computations.
  -- ( ) Without help for reward computations.
  -- ( ) Critic learning rate. Is it possible to overfit?
  - ( ) Episodic rollouts with random resets between two rollouts.
  - (x) Add "best boards":
  -- (x) Encoding-decoding transformer.
  -- (x) Pytest for best-boards internal state.
  - (x) Reset from current best boards.
  - ( ) Use a CNN as a backbone encoder. Concat best tiles with tiles. Use as many
        groups are there are total sides (8) before merging sides. How to use `n_steps` ?
  - ( ) Checker https://arxiv.org/abs/2110.00641
  - ( ) Regarder s'il est possible d'optimiser l'env batché

** MCTS
   - (x) Renommer "select_leafs"
   - (x) Ajouter un réel "select_leafs"
   - (_) Simuler un rollout
   -- ( ) Utiliser le value network pour évaluer un node.
   - (x) Backprop les résultats des rollouts
   - ( ) Gérer les noeuds des envs terminés
   - ( ) Gérer les rewards (évaluation du plateau à la fin du rollout?)
   - (_) Faire des steps dans le rollout qui sont le nombre maximum de steps pour
         finir le rollout de chaque env
   - ( ) Gérer la valeur des nouveaux fils à l'aide du value network
   - ( ) S'inspirer d'AlphaZero pour l'évaluation des candidats. Notamment prendre
         en compte la probabilité de chaque node.
   - ( ) Updater le meilleur puzzle trouvé pendant le tree search.

   - Comment backprop les résultats d'un rollout ?
   -- La somme pour commencer.
   -- On pourrait aussi prendre le max des childs.

* Why it does not work?
  - Is that possible that the model struggles at some point because the value function
    has overfit the model capacity?
  - (x) Is that possible that the values are not good at some point because the model
    is rewarded at the end of the rollout for the value of the final state.
  - (x) This means that at some point the model is able to reach a very good state
    but the value function does not recognize this and is learning that good moves
    values bad. THIS LOOKS GOOD!
  - Maybe use a composite reward ? Deltas rewards could help for credit assignment.
  - The issue is that we cannot give the value of the best previous state to
    the value network will struggle to not overfit onto it and collapse. And we should
    help the value network in some way? (should we?)
  - Maybe do smaller rollouts and do not reset all envs between two rollouts.
    This would allow for better credit assignment.

* Enhancements
  - Rotation and symmetry invariant.
  - 2D RoPE.

* Questions
  - Is the model better with conv layers, transfo layers, or little bit of both?
  -- Should I use positional encoding or are the conv layers enough?
  -- Ended up using transformer layers only, to have a simpler architecture and to
     make sure the model can easily attend to long interactions.
  - Does the perf scale with model size really?
  - Is curriculum learning helpful?
  - Should I let the model train longer?
  - Does random instances help?
  - Is it important to randomly reset the instances?
  - How to escape from the final plateau?
  - How to force the model to output different actions.
  -- Search strategies.
  - Episodic vs never-ending episodes.
  - CNN vs Transformer backbone.

* Ideas
  - Do MCTS once the model is trained and see if it is better than the model alone.
  - Use MC sampling to sample the actions based on the result of the multiple simulations.
  - Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many different games in parallel, and to make the search over and over.
  - Could it be possible to coordinate the search among the batch? A kind of multi-agent
    search. Avoid states that are being searched by others.
  -- ex: VAE giving a sense of how close two game states are. By the end of a rollout
     I can reward agents that have looked away from other agents, and that have not
     stayed in the same states during its rollout.
  - Better weight the entropy action loss.
  -- The first action can have great entropy, the last two should not!
  - Use a rotation and symmetry invariant model.
  - Use 2D RoPE.
  - Do not share parameters between actor and critic models.
  - Asymmetric self-play
  -- An agent is trained to challenge another agent to solve the puzzle.
  -- The first agent starts from a random solved puzzle, and mixes the pieces however it wants.
  -- The second agent tries to solve it back.
  -- The second agent is also trained from time to time on the real target puzzle.
  -- The games should gradually increase in difficulty by doing so.
  -- See {https://arxiv.org/abs/1703.05407}.
  - Add tricks from {https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}.
  - Construct artificial envs (so that we know the target state) and use this paper to
    learn a policy that solves it: {https://arxiv.org/abs/2310.06794v1}.
  - Ideally the model needs an architectural bias that allows him
    to play multiple rounds in its latent space.

** Batched MCTS
   - If we know the number of simulations we're doing, let's say N.
   - We can represent a tree by an array of shape [N, N].
     The first dims are all the nodes, the second are the childs ids of the given node.
   - We can have a null-node, the first one for example (id 0).
     This means that when we see that a node has the child node 0, it means the null node (empty child).
   - When iteratively visiting the nodes to find the node to select and expand,
     it is possible to do it iteratively accross the batch of trees (shape of [B, N, N]).
     Just iterate until everyone has reach the leaf.
