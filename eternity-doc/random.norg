@document.meta
title: random
description: Some weird facts
authors: pierrotlc
categories: 
created: 2023-06-04
updated: 2023-06-04
version: 1.1.1
@end


* Random thoughts
** Computing the logprobs with logits and softmax
   It is not the same to compute the logprobs from the outputted logits
   and from the softmax distribution.

   @code python3
   distrib = torch.distributions.Categorical(logits=logits)
   logprobs = distrib.log_prob(actions)
   @end

   @code python3
   distrib = torch.distributions.Categorical(probs=torch.softmax(logits))
   logprobs = distrib.log_prob(actions)
   @end

   The second one is the one that should be used.
   I think that the first one is either wrong or inefficient.

** Instability in the training
   The reward normalization can lead to instabilities in the training.
   It is important to take this into account and to use a rather large epsilon.

** Actor/critic
   My implementation seems to not be working so well, or at most to do
   equally as the base implementation. It may be because the current reward
   estimation is already good enough thanks to the high batch MC sampling.

   It also introduces a lot of hyperparameters to tune, so I decided to
   remove it.

** Reward shaping
   The 'win' reward is good because it does not optimize for the number of
   steps to win, nor for intermediate easy rewards. But it has a problem:
   long rollouts tend to all collapse to the mean state. The mode is not
   rewarded for the maximum intermediate state.
