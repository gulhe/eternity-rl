@document.meta
title: random
description: Some weird facts
authors: pierrotlc
categories: Notes
created: 2023-06-04
updated: 2023-07-30
version: 1.1.1
@end


* Random thoughts
** Computing the logprobs with logits and softmax
   It is not the same to compute the logprobs from the outputted logits
   and from the softmax distribution.

   @code python3
   distrib = torch.distributions.Categorical(logits=logits)
   logprobs = distrib.log_prob(actions)
   @end

   @code python3
   distrib = torch.distributions.Categorical(probs=torch.softmax(logits))
   logprobs = distrib.log_prob(actions)
   @end

   The second one is the one that should be used.
   I think that the first one is either wrong or inefficient.

** Instability in the training
   The reward normalization can lead to instabilities in the training.
   It is important to take this into account and to use a rather large epsilon.

   A recurrent policy is also hard to train since it can lead to huge BPTT
   gradient updates. To counter this, I regularly stop the gradient during the rollout.

** Actor/critic
   My implementation seems to not be working so well, or at most to do
   equally as the base implementation. It may be because the current reward
   estimation is already good enough thanks to the high batch MC sampling.

   It also introduces a lot of hyperparameters to tune, so I decided to
   remove it.

   Finally, I have to use it back since i'm training in a never-ending environment.
   To have access to a return estimation, I have to estimate the values using a value function.


** Reward modeling
   The `win` reward is good because it does not optimize for the number of
   steps to win, nor for intermediate easy rewards. But it has a problem:
   long rollouts tend to all collapse to the mean state. The mode is not
   rewarded for the maximum intermediate state.

   The new reward called `max-cut` provides a more stable training, and is
   way more sample efficient.

** Rollout buffer
   The rollout buffer is primarily used to be more sample efficient by reusing past
   samples during the training. But in my case, collecting the rollout samples is
   pretty fast so that I can do without such a buffer.

   Being able to train without a rollout buffer is nice because it makes it easier
   to implement tricky policies such as RNNs (useful for planning).

** Rotation equivariant
   The game is strictly rotation equivariant, meaning that the model should output
   the same rotated actions to a rotated board. This can be integrated in the algorithm
   in multiple ways:
   ~ Properly randomize the board state, so that the model sees a lot of diverse board
     states and learns to be equivariant by itself.
   ~ Add a regularization loss that directly enforce the model to be equivariant to
     rotations of the board.
   ~ Design the model so that it is naturally equivariant to rotations.

   Note that the game is also strictly equivariant to horizontal and vertical flips.

   The best thing to do is to design the model to be equivariant to rotations and flips.
   To do so, the encoder can use a {https://arxiv.org/abs/1602.07576}[group equivariant CNN]
   and/or a simple transformer encoder (with a symmetrical equivariant positional encoding, if used).
   The decoder can use a pointer network to select the tile to swap, and since the rolls
   are relative to the current tile placement, it can be done with a simple linear layer.

** Parallel exploration
   Using pure MCTS is not cool because it is hard to implement in parallel. Another way
   of exploring potential futures and to plan is to duplicate each instance and to do
   batched steps forward. It can be used to estimate what the next best action is.
   This process can also be repeated multiple times to trade time for memory.

** Potentially never-ending episodes
   A proper way to train the model is to use never-ending episodes. Doing so,
   the return is impossible to truly compute with MCTS sampling, but it can
   be estimated with a value network.

   But it is hard to train a value network in my environment since the model
   can kind of go back in time. By switch pieces it is easy to go to a previously
   encountered state. The value network has to take into account this fact, and
   the training becomes harder.

   The reward is also very important. A good reward shaping will make it easier
   for the model to quickly understand what the goal is.
