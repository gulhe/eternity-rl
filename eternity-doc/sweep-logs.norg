@document.meta
title: Sweep Logs
description: Logs of my experiments and thougths.
authors: pierrotlc
categories: logs
created: 2023-03-29
updated: 2023-03-31
version: 1.0.0
@end

* Experiment: find good starting search spaces
  The overall goal is to explore a little the search spaces
  and get a feeling of what is good and what is not.

** Sweep 1
   Goal: A first feeling about the following HPs:
   - `advantage` type (`learned`, `estimated` and `no-advantage`).
   - `gamma` between $0.9$ and $1.0$.

   To have a somehow challenging environment without having to run
   too many episodes, I decided to use the `trivial_B` environment.

   But most of the sweeps was really bad, and after a quick analysis
   I found out that the learning rate was too high which probably
   led to unstable learning.

** Sweep 2
   A refined version of the first sweep, with a lower learning rate
   range, between $0.0001$ and $0.001$.

   Doing so was much better, but the runs with the `learned` advantage
   was learning really slowly. I figured out that the `value_weight`
   is too high, which led to a value network taking most of the gradient
   norm.

** Sweep 3
   To better compare the `learned` advantage with the other from {Sweep 2},
   I refined the `value_weight` range between $0.01$ and $0.1$.

   As expected, the runs are much better. But I do not detect a particular
   good range of values within the new search space.
   I suspected that the good and bad runs were just pure luck, so I retrained
   the best and worst runs from this sweep.
   Indeed, the best run didn't maintain its good performance across all runs,
   and the worst run has improved a little on some of its runs, making it comparable
   to the previous runs.

** Conclusion
   I did narrow the search space of the `learning_rate` and `value_weight`
   and I found that the runs performances were pretty random once the HPs
   have been set. This calls for a better training loop, to get a stable (reproducible)
   training loop.
