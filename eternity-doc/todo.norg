@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2023-06-14
@end

* TODO
  - ( ) Explore the importance of the multiple HPs.
  -- ( ) Timesteps
  -- ( ) Model init.
  -- ( ) MLP for timesteps processing.
  - (=) Mixing MCTS with neural networks -> AlphaZero.
  -- (=) Is it possible to implement it in parallel on GPU?
  - (x) Add critic loss to the actor loss.
  -- It is not working so well, maybe because i'm doing MC sampling,
     or maybe because i'm doing it wrong.
  - (x) Reward function not sensible to potential bad ending moves.
  -- (_) Curriculum learning?
  - (x) Better best state saving.
  - (x) Remove MLP.
  - (x) Use argmax sampling when evaluating the model.
  - ( ) Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many differents games in parallel, and to make the search over and over.
  -- It may be possible to implement this on multiple GPUs?
