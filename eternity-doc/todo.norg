@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2023-06-23
@end

* TODO
  - (x) Implement a transformer policy.
  -- (_) Maybe use a dedicated token to sumup the state of the game.
  -- (_) If not, maybe a pointer network?
  -- (x) Add a token for time encoding.
  -- ( ) Make the cross attention happen between each action.
  - ( ) Make sure that the RNN is able to know what was the last selected action.
  - ( ) Add a next state prediction loss.

* Ideas
  - Do MCTS once the model is trained and see if it is better than the model alone.
  - Do a deeper head for each separate action.
  - Use MC sampling to sample the actions based on the result of the multiple simulations.
  - Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many different games in parallel, and to make the search over and over.
  -- It may be possible to implement this on multiple GPUs?
