@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2023-07-16
@end

* TODO
  - ( ) Add a rotation invariant regularizer loss (if not using a GNN or G-CNN).
  - ( ) Cut the rollout based on the last minimum score (remove the first useless moves).
  - (x) Start with a CNN encoder for local embeddings and finish by a transformer encoder
        for global embeddings.
  - (x) Go back to a rollout buffer, remove the recurrent model and add the optional
        timestep encoding.
  - (x) Do a special rollout by using a batched soft MCTS (see {*Ideas}) to collect better actions.
  -- (x) It will be much longer to collect rollouts, hence the use of a rollout buffer.
  -- (x) Possibility to add exploration by using exploration sampling methods.
  -- (x) Merge multiple simulations of a single instance by batching copies of instances.
  -- The training perfs start by going down and then goes up once the model learns
     a good policy. It could be because after the first moves are with a depth of one,
     with a bad policy, which tells better greedy moves than a bad policy with a bigger depth
     which averages the bad overall moves. It could also be because once the model learns,
     it lowers its exploration, leading to a bad MCTS.
  - ( ) Start some rollouts from previous best boards.
  -- Or simply sample from previous boards and completely new boards.
  -- It would allow for smaller steps.
  -- Needs to adapt the reward shaping.
  - Use a better rollout buffer, sample more and rollout less.
  - Use a value network to quickly estimate the value of a state => shallow MCTS!

* Ideas
  - Do MCTS once the model is trained and see if it is better than the model alone.
  - Do a deeper head for each separate action.
  - Use MC sampling to sample the actions based on the result of the multiple simulations.
  - Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many different games in parallel, and to make the search over and over.
  -- It may be possible to implement this on multiple GPUs?
  - Do a continuously learning model.
  -- It could just play forever and constantly learn from its previous actions.
  - Instead of using an experience replay buffer, why don't I just play multiple subgames
    for each sample of the batch?
  -- Since playing a game is computionally light, it may be possible to play a lot of games
     in parallel.
  -- This is like a replay buffer, but with fresh samples (from the current policy) each time.
