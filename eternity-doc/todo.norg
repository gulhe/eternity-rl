@document.meta
title: todo
authors: pierrotlc
categories: notes
created: 2023-03-08
updated: 2023-07-05
@end

* TODO
  - (x) Implement a transformer policy.
  -- (_) Maybe use a dedicated token to sumup the state of the game.
  -- (_) If not, maybe a pointer network?
  -- (x) Add a token for time encoding.
  -- ( ) Make the cross attention happen between each action.
  - (x) Make sure that the RNN is able to know what was the last selected action.
  - (x) Add a next state prediction loss.
  -- (x) Or simply add a RNN to the model.
     Do take care to implement a proper experience replay. Replay the last $n$ steps of the game.
  - (-) Try out a simple GNN, which is rotation invariant.
  -- ( ) TokenGT maybe?
  -- (x) To enable the node selection, we can use attention mechanism to have a probability
     over the nodes.
  -- (x) To incorporate the information of the last selected node, we can add a specicial
     embedding to that node. Same thing for the selected shift.
  -- (x) We can use the decoder with a single query, and use it each time we have to predict
     an action. It means that each time we have to predict an action, it will observe
     the embedded tiles and base its prediction on that.
  - ( ) Add a rotation invariant regularizer loss (if not using a GNN).
  - ( ) Cut the rollout based on the last minimum score (remove the first useless moves).

* Ideas
  - Do MCTS once the model is trained and see if it is better than the model alone.
  - Do a deeper head for each separate action.
  - Use MC sampling to sample the actions based on the result of the multiple simulations.
  - Once a model is trained, it is possible to do a soft MCTS:
  ~~ Start from a random state.
  ~~ Duplicate the state into many (many) batch games.
  ~~ Do a rollout with random sampling for each games in the batch.
  ~~ Aggregate the final results and apply the actions that led to the best final (maximum) state.
  ~~ Go back to point B
  -- This should enhance the overall search for a solution. It may be possible to apply this search
     with many different games in parallel, and to make the search over and over.
  -- It may be possible to implement this on multiple GPUs?
  - Do a continuously learning model.
  -- It could just play forever and constantly learn from its previous actions.
  - Instead of using an experience replay buffer, why don't I just play multiple subgames
    for each sample of the batch?
  -- Since playing a game is computionally light, it may be possible to play a lot of games
     in parallel.
  -- This is like a replay buffer, but with fresh samples (from the current policy) each time.
